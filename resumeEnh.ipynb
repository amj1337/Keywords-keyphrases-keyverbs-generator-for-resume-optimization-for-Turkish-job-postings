{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5fc087",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b560e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cde867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238c9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fa9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bc2df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\The\n",
      "[nltk_data]     Creator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa09bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635be4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09b49",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cead6a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#scraping the website using Selenium web driver and BeautifulSoup for the total number of pages of job postings for the specified title\n",
    "def getTotalpages(jobTitle):\n",
    "    url = 'https://www.kariyer.net/is-ilanlari?kw=' + jobTitle #creating the full URL for the job postings search page\n",
    "#creating an instance of the Firefox webdriver(the instructions of use for each browser are simple and can be found in the Selenium documentation)\n",
    "    driver = webdriver.Firefox(executable_path = r'./geckodriver.exe') \n",
    "    driver.get(url) #navigating the webdriver to the specified URL\n",
    "    page = driver.page_source #extracting the page source of the current webdriver page\n",
    "    soup = BeautifulSoup(page, 'html.parser') #parsing the HTML of the page\n",
    "    totPages = 0\n",
    "    for b in list(l.find('button') for l in list(li for li in soup.find('ul', {'aria-label': 'Pagination'}).find_all('li', {'role': 'presentation'}))):\n",
    "        if b:\n",
    "#getting the total number of pages situated at the 'aria-setsize' attribute of the 'li' element in the page source code\n",
    "            totPages = int(b['aria-setsize']) \n",
    "            break\n",
    "    driver.close()\n",
    "    return totPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7937a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting URLs of job postings for the specified position\n",
    "def jobUrls_kariyer(jobTitle,totalPages):\n",
    "    if totalPages != 1:\n",
    "        for pNmbr in range(1,totalPages + 1):\n",
    "#delaying the execution of the program by 1 second for each iteration in order to avoid slowing down the website\n",
    "            time.sleep(1)         \n",
    "            url = 'https://www.kariyer.net/is-ilanlari?kw=' + jobTitle + '&cp=' + str(pNmbr)\n",
    "            driver = webdriver.Firefox(executable_path = r'./geckodriver.exe')\n",
    "            driver.get(url)\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "#finding all the anchor tags with the class name 'k-ad-card rounded' that contain the job listings\n",
    "            jListings = soup.find_all('a', class_ = 'k-ad-card rounded')\n",
    "#creating a list of URLs for each job listing by extracting the href attribute of each anchor tag and concatenating it with the base URL\n",
    "            jListingsRefs = ['https://www.kariyer.net/' + ref['href'] for ref in jListings] \n",
    "            driver.close()\n",
    "    else:\n",
    "            time.sleep(1)        \n",
    "            url = 'https://www.kariyer.net/is-ilanlari?kw=' + jobTitle\n",
    "            driver = webdriver.Firefox(executable_path = r'./geckodriver.exe')\n",
    "            driver.get(url)\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            jListings = soup.find_all('a', class_ = 'k-ad-card rounded')\n",
    "            jListingsRefs = ['https://www.kariyer.net/' + ref['href'] for ref in jListings]\n",
    "            driver.close()\n",
    "    return jListingsRefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81aaac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all job descriptions from each URL of the job postings that were previously extracted\n",
    "def jobDescriptions(listings):\n",
    "    jobDescriptions = []\n",
    "    for url in listings: \n",
    "        jPage = requests.get(url)\n",
    "        jSoup = BeautifulSoup(jPage.content, 'html.parser')\n",
    "        jDescription = jSoup.find_all('div' , attrs={'class' : 'genel-nitelikler' }) \n",
    "        for jD in jDescription:\n",
    "            if 'GENEL NİTELİKLER VE İŞ TANIMI' in jD:\n",
    "                jD = jD.replace('GENEL NİTELİKLER VE İŞ TANIMI','')\n",
    "            if 'QUALIFICATIONS AND JOB DESCRIPTION' in jD:\n",
    "                jD = jD.replace('QUALIFICATIONS AND JOB DESCRIPTION','')\n",
    "            jobDescriptions.append(jD.text)\n",
    "        time.sleep(1) \n",
    "    return jobDescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6262efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing digits\n",
    "def removeDigits(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyzğıöşçüâ'\n",
    "    for letter in word: #iterating through each letter of a word looking for digits to remove\n",
    "        if letter not in letters:\n",
    "            word = word.replace(letter,'') \n",
    "    return(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f07c32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing and cleaning the data\n",
    "def preprocess(jobDescriptions):\n",
    "    njd = []\n",
    "    oneTxt = ''\n",
    "    stopWords = stopwords.words('turkish') #turkish stopwords from nltk\n",
    "    nstopWords = ['veveya','vb','vs','yıl','e','bana', 'ar', 'ge', 'x','arıyoruz', '', 'ten', 'com', 'ş', 'ch', 'abl', 'mak', 'fon', 'hakkında', 'bilgi', 'sahibi', 'konusunda '] #noise words\n",
    "    with open('./turkce-stop-words.txt','r', encoding='utf-8') as trstopwordsFile :\n",
    "#importing turkish stopwords from Ahmet Aksoy's trstop project(link: https://github.com/ahmetax/trstop/blob/master/dosyalar/turkce-stop-words)\n",
    "        trstopwords = trstopwordsFile.read() \n",
    "    trstopwords = trstopwords.split('\\n')\n",
    "#importing english stopwords from nltk because some job postings might be written in english\n",
    "    enstopWords = stopwords.words('english')\n",
    "#grouping all stopswords found\n",
    "    stopWords.extend(nstopWords)\n",
    "    stopWords.extend(enstopWords)\n",
    "    stopWords.extend(trstopwords)\n",
    "    stopWords = list(set(stopWords)) #getting rid of duplicate stopwords\n",
    "\n",
    "#joing all job descriptions into one text in order to be tokenized later on  \n",
    "    oneTxt = ' '.join([description for description in jobDescriptions])\n",
    "    oneTxt = oneTxt.strip() #removing any leading or trailing spaces\n",
    "#replacing special charcters with white spaces because some suffixes might be joined with words\n",
    "    for char in ',/:()[]{}#*-+.&;':\n",
    "        if char in oneTxt:\n",
    "            oneTxt = oneTxt.replace(char, ' ')\n",
    "\n",
    "#tokenizing the text(nltk tokenize could have been used but wouldn't have made much difference in this case scenario)\n",
    "    tokens = oneTxt.split()\n",
    "\n",
    "    for token in tokens:\n",
    "        token = removeDigits(token.lower()) #getting rid of digits\n",
    "        #token = token.strip() #removing any leading or tariling spaces(some words might have theme since )\n",
    "#cleaning the tokenized words from any stopwords/noise and empty strings resulted from using split\n",
    "        if token != '' and token not in stopWords: \n",
    "            njd.append(token) #grouping the tokenized and cleaned words into list\n",
    "    return njd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e36ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the occurrences of each word in the set of job descriptions\n",
    "def occurrences(tokens):\n",
    "    freq = {}\n",
    "    for token in tokens:\n",
    "        freq[token] = tokens.count(token)\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f46b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#claculating the term frequency–inverse document frequency score for each term present in the job descriptions\n",
    "def tfidf(term,jds):\n",
    "    tfs = [jd.count(term) / len(jd) for jd in jds] #list of term frequency values in each job description\n",
    "    df = sum(term in jd for jd in jds) #document frequency of the term\n",
    "    idf = math.log(len(jds) / (1 + df)) #inverse document frequency of the term\n",
    "    tfIdf = [tf * idf for tf in tfs] #tf-idf score of the term\n",
    "    return sum(tfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8582921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating threshold value and filtering out terms with a tf-idf score equal and lower than the threshold\n",
    "def thresholdFilter(scores):\n",
    "    mean = statistics.mean(set(scores.values()))\n",
    "    std = statistics.stdev(set(scores.values()))\n",
    "#setting a threshold value at two standard deviations above the mean(the std multiplier is chosen by experimenting with different values)\n",
    "    for key,val in list(scores.items()):\n",
    "        thresholdScore = mean + 2*std \n",
    "        if val < thresholdScore:\n",
    "                scores.pop(key)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c95c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating ngrams\n",
    "def ngrams(tokonizedJdescs):\n",
    "    n = 1\n",
    "    totNgrams = {}\n",
    "    ngrams = []\n",
    "    while True:\n",
    "        ngrams = list((' '.join(tokonizedJdescs[i:i+n]) for i in range(len(tokonizedJdescs) - n + 1))) \n",
    "        totNgrams[str(n)+'grams'] = occurrences(ngrams) #using the number of occurrences to set the n at which it should stop\n",
    "        if len(ngrams)/len(occurrences(ngrams).keys()) > 1: #generating ngrams until the occurrence of each ngram is 1\n",
    "            n += 1\n",
    "            ngrams = []\n",
    "        else:\n",
    "            break\n",
    "#deleting the last set of ngrams because they will all have an occurrence of 1\n",
    "    del totNgrams[list(totNgrams.keys())[-1]] \n",
    "    return totNgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "807da0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer\n",
    "def verbLemmatizer(tokonizedJdescs,JobDescriptions):\n",
    "#importing TDK's(Turkish Language Association) dictionary words from Necmettin Çarkacı's TDKDictionaryCrawler project(link: https://github.com/ncarkaci/TDKDictionaryCrawler)\n",
    "    with open('./TDK_Sözlük_Kelime_Listesi.txt', 'r', encoding='utf-8') as tdkDic:\n",
    "        tdkWords = tdkDic.read().split()\n",
    "#filtering out non verbal words\n",
    "#all infinitive Turkish verbs end with 'mek' or 'mak' but not all words that end with 'mak' or 'mek' are verbs, that's why I had to create a list of those exceptions from the words I know and the ones I was able to find\n",
    "    tdkVerbs = [word.lower() for word in set(tdkWords) if (word.endswith(('mek','mak')) and word not in['kaymak','ekmek','emek','yemek','ahmak', 'yamak', 'sümek', 'hamak', 'somak', 'temek', 'pomak', 'temek', 'mamak', 'sumak', 'ramak', 'kuymak', 'yumak'])] \n",
    "    tdkVerbroots = [verb[:-3] for verb in set(tdkVerbs)] #getting the roots of verbs found in the dictionary\n",
    "#importing an AI generated alphabetically sorted list of Turkish verbal suffixes\n",
    "    with open('./TR_verbal_suffixes.txt','r', encoding='utf-8') as suffixesFile:\n",
    "        suffixes = suffixesFile.read().split()\n",
    "#identifying potential verbs from the words extracted from job descriptions by looking verbal suffixes to filter them out\n",
    "    potentialVerbs = []\n",
    "    for word in set(tokonizedJdescs):\n",
    "        for suffix in set(suffixes):\n",
    "            if suffix in word:\n",
    "                potentialVerbs.append(word)\n",
    "                break\n",
    "#finding roots of verbs present in job descriptions\n",
    "    roots = {}\n",
    "    for word in potentialVerbs:\n",
    "        for root in tdkVerbroots:\n",
    "            if root == word[:len(root)]: #conjugated verbs in the Turkish language always have suffixes added after the verb root\n",
    "                roots[word] = root #no break because some roots are ranked before other roots that might be more relevant\n",
    "#sorting verbs based on their tf-idf scores\n",
    "    verbs ={}\n",
    "    for word in roots.keys():\n",
    "        if tdkVerbs[tdkVerbroots.index(roots[word])] in verbs:\n",
    "            verbs[tdkVerbs[tdkVerbroots.index(roots[word])]] += tfidf(word,JobDescriptions)\n",
    "        else:\n",
    "            verbs[tdkVerbs[tdkVerbroots.index(roots[word])]] = tfidf(word,JobDescriptions)\n",
    "    verbs = dict(sorted(verbs.items(), key = lambda x:x[1], reverse = True))\n",
    "    verbs = {key:val for key,val in verbs.items() if val} #filtering out verbs with a score of 0.0\n",
    "    verbs = thresholdFilter(verbs) #filtering out verbs that are below the thereshold\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cfb53",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5234dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job listings urls \n",
    "title = 'grafik tasarımcı'\n",
    "pages = getTotalpages(title)\n",
    "jobListings = jobUrls_kariyer(title,pages)\n",
    "if not jobListings:\n",
    "    print('Aradiginiz is ilani bulunamadi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4a163a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobDescs = jobDescriptions(jobListings) #job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "214864c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokonizedcleanJdescs = preprocess(jobDescs) #tokenized and cleaned job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c29b3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "allNgrams = ngrams(tokonizedcleanJdescs) #all possible ngrams from 1 to n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c3a082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramsASkeys = list(allNgrams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5905fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngramsASkeys:\n",
    "    for key in allNgrams[ngram]:\n",
    "        allNgrams[ngram][key] = tfidf(key,jobDescs) #tf-idf scores of all ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60f2e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngramsASkeys:\n",
    "    if len(set(allNgrams[ngram].values())) < 2:\n",
    "        del allNgrams[ngram]\n",
    "    else:\n",
    "        allNgrams[ngram] = thresholdFilter(allNgrams[ngram])\n",
    "allNgrams = {key:val for key,val in allNgrams.items() if val} #ngrams that are above the threshold score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da9a1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramsASkeys = list(allNgrams.keys()) #keys of ngrams for easier navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a219832",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = dict(sorted(allNgrams[ngramsASkeys[0]].items(), key = lambda x:x[1], reverse = True)) #sorted unigrams\n",
    "keyterms = dict(sorted(allNgrams[ngramsASkeys[1]].items(), key = lambda x:x[1], reverse = True)) #sorted bigrams\n",
    "multigrams = [allNgrams[ngramsASkeys[i]] for i in range(2,len(ngramsASkeys))] #the rest of the ngrams\n",
    "keyphrases = {}\n",
    "for dic in multigrams:\n",
    "    for key, val in dic.items():\n",
    "        keyphrases[key] = val\n",
    "keyphrases = dict(sorted(keyphrases.items(), key = lambda x:x[1], reverse = True)) #sorted phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57624cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbs present in job descriptions and brought to their infinitive form\n",
    "keyverbs = verbLemmatizer(tokonizedcleanJdescs,jobDescs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896735a",
   "metadata": {},
   "source": [
    "# Program output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe317df",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5e003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c3f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b0c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfda5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f0c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
